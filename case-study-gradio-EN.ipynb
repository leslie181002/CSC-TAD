{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219277df-137e-48c6-869b-6bc7a417f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeslieHailee\\AppData\\Local\\Temp\\ipykernel_15676\\723856475.py:8: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "â³ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeslieHailee\\anaconda3\\envs\\finetuning\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.842 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4dbfffe7dd41fab4d4140da6235295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully\n",
      "âš™ï¸ Creating inference pipeline...\n",
      "ğŸš€ Launching Gradio interface...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources cleaned up\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Configuration settings\n",
    "base_model_name = \"unsloth/Qwen3-14B\"\n",
    "adapter_path = \"\"\n",
    "output_dir = r\"\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_csv_path = os.path.join(output_dir, \"supply_chain_accidents_data.csv\")\n",
    "\n",
    "# Load model and tokenizer (fixed device issue)\n",
    "def load_model():\n",
    "    print(\"â³ Loading model...\")\n",
    "    try:\n",
    "        base_model, _ = FastLanguageModel.from_pretrained(\n",
    "            model_name=base_model_name,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            adapter_path,\n",
    "            adapter_name=\"accident_cause_adapter\"\n",
    "        )\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            adapter_path,\n",
    "            padding_side=\"left\",\n",
    "            truncation_side=\"left\"\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"âœ… Model loaded successfully\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model loading failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Create inference pipeline (fixed device error)\n",
    "def create_pipeline(model, tokenizer):\n",
    "    if model is None or tokenizer is None:\n",
    "        return None\n",
    "        \n",
    "    print(\"âš™ï¸ Creating inference pipeline...\")\n",
    "    try:\n",
    "        # Remove device parameter, use accelerate's automatic device management\n",
    "        return pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline creation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Global variables to store model instances\n",
    "model, tokenizer = load_model()\n",
    "pipe = create_pipeline(model, tokenizer)\n",
    "\n",
    "# Save results to CSV\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame([data])\n",
    "    if not os.path.exists(output_csv_path):\n",
    "        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ğŸ“„ Created new CSV file: {output_csv_path}\")\n",
    "    else:\n",
    "        df.to_csv(output_csv_path, mode='a', header=False, index=False, encoding='utf_8_sig')\n",
    "        print(f\"ğŸ“ Appended data to CSV file\")\n",
    "\n",
    "# Parse model response\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        # Extract thought chain\n",
    "        think_pattern = r\"<think>(.*?)</think>\"\n",
    "        think_match = re.search(think_pattern, response, re.DOTALL)\n",
    "        think_chain = think_match.group(1).strip() if think_match else \"æœªæ‰¾åˆ°æ€ç»´é“¾\"\n",
    "        \n",
    "        # Extract direct cause\n",
    "        direct_cause_pattern = r\"ç›´æ¥åŸå› :\\s*(.*?)(?:\\n|$)\"\n",
    "        direct_match = re.search(direct_cause_pattern, response)\n",
    "        direct_cause = direct_match.group(1).strip() if direct_match else \"æœªæ‰¾åˆ°ç›´æ¥åŸå› \"\n",
    "        \n",
    "        # Extract indirect cause\n",
    "        indirect_cause_pattern = r\"é—´æ¥åŸå› :\\s*(.*?)(?:\\n|$)\"\n",
    "        indirect_match = re.search(indirect_cause_pattern, response)\n",
    "        indirect_cause = indirect_match.group(1).strip() if indirect_match else \"æœªæ‰¾åˆ°é—´æ¥åŸå› \"\n",
    "        \n",
    "        return think_chain, direct_cause, indirect_cause\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "        return \"è§£æé”™è¯¯\", \"è§£æé”™è¯¯\", \"è§£æé”™è¯¯\"\n",
    "\n",
    "# Generate model response\n",
    "def generate_response(input_text):\n",
    "    if pipe is None:\n",
    "        return \"æ¨¡å‹æœªåŠ è½½\", \"æ¨¡å‹æœªåŠ è½½\", \"æ¨¡å‹æœªåŠ è½½\", 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä¸ªäº‹æ•…å› æœæ¨ç†ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®inputä¸­çš„äº‹æ•…è¿‡ç¨‹ä»ç»™å‡ºçš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› åˆ†ç±»è¡¨ä¸­æ¨ç†å‡ºäº‹æ•…å¯¹åº”çš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› ï¼Œè¾“å‡ºåªèƒ½ä¸ºï¼šæ€ç»´é“¾ã€‚ç›´æ¥åŸå› :ç›´æ¥åŸå› åˆ—è¡¨ï¼Œé—´æ¥åŸå› :é—´æ¥åŸå› åˆ—è¡¨ã€‚ç›´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„,è¶…é€Ÿä¸è½½é‡åˆè§„æ€§,é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„,é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ,è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†,è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ,å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ.é—´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨,è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ,é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½,ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆã€‚\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    formatted_prompt = prompt_template.format(input_text)\n",
    "    \n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 30,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        outputs = pipe(\n",
    "            formatted_prompt,\n",
    "            **generation_config,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        response = outputs[0]['generated_text'].strip()\n",
    "        \n",
    "        # Clean response content\n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        response = re.sub(r'(ç›´æ¥åŸå› |é—´æ¥åŸå› ).*?[:ï¼š]', '', response, count=1)\n",
    "        \n",
    "        # Parse response\n",
    "        think_chain, direct_cause, indirect_cause = parse_response(response)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Save results\n",
    "        save_to_csv({\n",
    "            \"äº‹æ•…æè¿°\": input_text,\n",
    "            \"æ€ç»´é“¾\": think_chain,\n",
    "            \"ç›´æ¥åŸå› \": direct_cause,\n",
    "            \"é—´æ¥åŸå› \": indirect_cause,\n",
    "            \"å“åº”æ—¶é—´(ç§’)\": round(elapsed_time, 2),\n",
    "            \"æ—¶é—´æˆ³\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        \n",
    "        return think_chain, direct_cause, indirect_cause, round(elapsed_time, 2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return f\"é”™è¯¯: {str(e)}\", \"\", \"\", round(elapsed_time, 2)\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks(title=\"Construction Supply Chain Accident Analysis System\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ğŸ—ï¸ Construction Supply Chain Accident Analysis System\")\n",
    "    gr.Markdown(\"### Enter accident description to analyze direct and indirect causes\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            accident_input = gr.Textbox(\n",
    "                label=\"Accident Description\",\n",
    "                placeholder=\"Please enter the description of the accident during the construction supply chain transportation phase...\",\n",
    "                lines=5,\n",
    "                max_lines=10\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Analyze Accident\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear Input\")\n",
    "            \n",
    "            with gr.Accordion(\"Example Inputs\", open=False):\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"A heavy-duty truck transporting construction piles lost control and overturned on a curve. The driver had been working continuously for 12 hours and was speeding. There were signs that the fixing devices for the five piles loaded on the vehicle were loose. Post-incident inspection revealed a 0.8-second delay in the braking system response, aging sensors causing deviations in braking force distribution, and evidence of illegal modifications to the onboard speed-limiting module.\"],\n",
    "                        [\"During unloading at a construction site, a crane operator failed to observe the surrounding area, resulting in a steel pipe falling and striking a worker operating nearby. The investigation found that the operator lacked formal training, the safety supervisor was absent from duty at the time, and the crane's safety limit device had malfunctioned.\"]\n",
    "                    ],\n",
    "                    inputs=accident_input\n",
    "                )\n",
    "        \n",
    "        with gr.Column():\n",
    "            think_output = gr.Textbox(label=\"Thought Chain Analysis\", interactive=False, lines=7)\n",
    "            with gr.Row():\n",
    "                direct_output = gr.Textbox(label=\"Direct Cause\", interactive=False)\n",
    "                indirect_output = gr.Textbox(label=\"Indirect Cause\", interactive=False)\n",
    "            time_output = gr.Number(label=\"Response Time (seconds)\", interactive=False)\n",
    "            status_output = gr.Textbox(label=\"System Status\", value=\"Ready\", interactive=False)\n",
    "    \n",
    "    # Submit processing\n",
    "    submit_btn.click(\n",
    "        fn=generate_response,\n",
    "        inputs=accident_input,\n",
    "        outputs=[think_output, direct_output, indirect_output, time_output]\n",
    "    ).then(\n",
    "        fn=lambda: \"Analysis Completed\",\n",
    "        outputs=status_output\n",
    "    )\n",
    "    \n",
    "    # Clear input\n",
    "    clear_btn.click(\n",
    "        fn=lambda: [\"\", \"\", \"\", \"\", 0, \"Cleared\"],\n",
    "        outputs=[accident_input, think_output, direct_output, indirect_output, time_output, status_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    **Instructions:**\n",
    "    1. Enter detailed accident description in the left input box\n",
    "    2. Click \"Analyze Accident\" button to get analysis results\n",
    "    3. Analysis results will be automatically saved to the system database\n",
    "    4. View standard input format through examples\n",
    "    \n",
    "    **Note:** Initial run requires model loading and may take 1-2 minutes\n",
    "    \"\"\")\n",
    "\n",
    "# Launch application\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"ğŸš€ Launching Gradio interface...\")\n",
    "        demo.launch(\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=7860,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Launch failed: {str(e)}\")\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"Resources cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8bff11-e7da-403a-afdc-d4a4d063478c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

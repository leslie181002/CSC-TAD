{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f7a29-0576-4a96-b8dc-69abaad99f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Enhanced Evaluation Model (Based on Code Pattern 2)\"\"\"\n",
    "# ===================== Additional Imports =====================\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "# ===================== New Definitions =====================\n",
    "# Define direct cause categories\n",
    "DIRECT_CAUSES = [\n",
    "    \"Insufficient vehicle operation safety awareness\",\n",
    "    \"Non-compliance with speed and load regulations\",\n",
    "    \"Non-compliant driving behavior in high-risk areas\",\n",
    "    \"Heavy vehicle operation standards\",\n",
    "    \"Vehicle stability management\",\n",
    "    \"Safe cargo handling procedures\",\n",
    "    \"Non-standard unloading operations\"\n",
    "]\n",
    "\n",
    "# Define indirect cause categories\n",
    "INDIRECT_CAUSES = [\n",
    "    \"Inadequate on-site personnel safety management\",\n",
    "    \"Failure in transportation compliance monitoring\",\n",
    "    \"Insufficient driver training\",\n",
    "    \"Corporate management failure in vehicle operations\"\n",
    "]\n",
    "\n",
    "# Cause extraction function (regex-based)\n",
    "def extract_causes(text):\n",
    "    \"\"\"Extract direct and indirect causes from output text using regex patterns\"\"\"\n",
    "    direct_match = re.search(r\"Direct Cause[:：]\\s*(.+)\", text)\n",
    "    indirect_match = re.search(r\"Indirect Cause[:：]\\s*(.+)\", text)\n",
    "    \n",
    "    direct = direct_match.group(1).strip() if direct_match else \"\"\n",
    "    indirect = indirect_match.group(1).strip() if indirect_match else \"\"\n",
    "    \n",
    "    # Split multiple causes\n",
    "    direct_list = [c.strip() for c in re.split(r\"[,，]\", direct) if c.strip()]\n",
    "    indirect_list = [c.strip() for c in re.split(r\"[,，]\", indirect) if c.strip()]\n",
    "    \n",
    "    return direct_list, indirect_list\n",
    "\n",
    "# Create multi-hot encoded vectors\n",
    "def create_multihot(labels, class_list):\n",
    "    \"\"\"Generate multi-hot encoded vector for cause classification\"\"\"\n",
    "    vector = np.zeros(len(class_list), dtype=int)\n",
    "    for label in labels:\n",
    "        if label in class_list:\n",
    "            idx = class_list.index(label)\n",
    "            vector[idx] = 1\n",
    "    return vector\n",
    "\n",
    "# ===================== Refactored Evaluation Function =====================\n",
    "def calculate_metrics(true_outputs, pred_outputs):\n",
    "    \"\"\"Enhanced metric calculation for cause analysis system\"\"\"\n",
    "    # Initialize data structures\n",
    "    true_direct_vectors = []\n",
    "    pred_direct_vectors = []\n",
    "    true_indirect_vectors = []\n",
    "    pred_indirect_vectors = []\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for true, pred in zip(true_outputs, pred_outputs):\n",
    "        # Extract causes\n",
    "        true_direct, true_indirect = extract_causes(true)\n",
    "        pred_direct, pred_indirect = extract_causes(pred)\n",
    "        \n",
    "        # Create multi-hot vectors\n",
    "        true_direct_vectors.append(create_multihot(true_direct, DIRECT_CAUSES))\n",
    "        pred_direct_vectors.append(create_multihot(pred_direct, DIRECT_CAUSES))\n",
    "        true_indirect_vectors.append(create_multihot(true_indirect, INDIRECT_CAUSES))\n",
    "        pred_indirect_vectors.append(create_multihot(pred_indirect, INDIRECT_CAUSES))\n",
    "        \n",
    "        # Prepare text for NLP metrics\n",
    "        references.append(true.split())\n",
    "        hypotheses.append(pred.split())\n",
    "    \n",
    "    # Calculate classification metrics (macro average)\n",
    "    metrics = {\n",
    "        \"direct_precision\": precision_score(true_direct_vectors, pred_direct_vectors, average=\"macro\", zero_division=0),\n",
    "        \"direct_recall\": recall_score(true_direct_vectors, pred_direct_vectors, average=\"macro\", zero_division=0),\n",
    "        \"direct_f1\": f1_score(true_direct_vectors, pred_direct_vectors, average=\"macro\", zero_division=0),\n",
    "        \"indirect_precision\": precision_score(true_indirect_vectors, pred_indirect_vectors, average=\"macro\", zero_division=0),\n",
    "        \"indirect_recall\": recall_score(true_indirect_vectors, pred_indirect_vectors, average=\"macro\", zero_division=0),\n",
    "        \"indirect_f1\": f1_score(true_indirect_vectors, pred_indirect_vectors, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Calculate BLEU (corpus-level)\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    metrics[\"bleu\"] = corpus_bleu(\n",
    "        [[ref] for ref in references],\n",
    "        hypotheses,\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "    \n",
    "    # Calculate ROUGE (average)\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(\n",
    "        [\" \".join(h) for h in hypotheses],\n",
    "        [\" \".join(r) for r in references],\n",
    "        avg=True\n",
    "    )\n",
    "    metrics.update({\n",
    "        \"rouge-1\": rouge_scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge-2\": rouge_scores[\"rouge-2\"][\"f\"],\n",
    "        \"rouge-l\": rouge_scores[\"rouge-l\"][\"f\"]\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ===================== Refactored Evaluation Workflow ===================== \n",
    "print(\"Starting model evaluation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize variables (assume test_dataset and model are defined elsewhere)\n",
    "test_inputs = test_dataset[\"input\"]\n",
    "true_outputs = test_dataset[\"output\"]\n",
    "pred_outputs = []\n",
    "\n",
    "# Setup progress tracking\n",
    "progress_bar = tqdm(total=len(test_inputs), desc=\"Generating predictions\", unit=\"sample\")\n",
    "\n",
    "# Generation parameters\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 20,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Prediction generation loop\n",
    "for i, input_text in enumerate(test_inputs):\n",
    "    try:\n",
    "        # Construct prompt (Alpaca format)\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an accident causation expert. Based on the accident process in the input, infer the direct and indirect causes from the provided classification tables. Output format must be: Reasoning chain. Direct Cause: list of direct causes, Indirect Cause: list of indirect causes.\n",
    "\n",
    "Direct Cause Classification Table:\n",
    "{', '.join(DIRECT_CAUSES)}\n",
    "\n",
    "Indirect Cause Classification Table:\n",
    "{', '.join(INDIRECT_CAUSES)}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract response section\n",
    "        if \"### Response:\" in pred_text:\n",
    "            pred_text = pred_text.split(\"### Response:\")[1].strip()\n",
    "        \n",
    "        pred_outputs.append(pred_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating prediction: {str(e)}\")\n",
    "        pred_outputs.append(\"\")\n",
    "    \n",
    "    # Update progress\n",
    "    progress_bar.update(1)\n",
    "    if (i + 1) % 5 == 0 and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Computing evaluation metrics...\")\n",
    "metrics = calculate_metrics(true_outputs, pred_outputs)\n",
    "\n",
    "# Print comprehensive results\n",
    "print(\"\\n===== Evaluation Results =====\")\n",
    "print(f\"Direct Cause Precision: {metrics['direct_precision']:.4f}\")\n",
    "print(f\"Direct Cause Recall: {metrics['direct_recall']:.4f}\")\n",
    "print(f\"Direct Cause F1: {metrics['direct_f1']:.4f}\")\n",
    "print(f\"Indirect Cause Precision: {metrics['indirect_precision']:.4f}\")\n",
    "print(f\"Indirect Cause Recall: {metrics['indirect_recall']:.4f}\")\n",
    "print(f\"Indirect Cause F1: {metrics['indirect_f1']:.4f}\")\n",
    "print(f\"BLEU Score: {metrics['bleu']:.4f}\")\n",
    "print(f\"ROUGE-1 F1: {metrics['rouge-1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {metrics['rouge-2']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {metrics['rouge-l']:.4f}\")\n",
    "print(f\"Total Time: {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "# Save structured results\n",
    "output_dir = \"evaluation_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "results_path = os.path.join(output_dir, \"advanced_evaluation_results.json\")\n",
    "\n",
    "results_data = {\n",
    "    \"metrics\": metrics,\n",
    "    \"predictions\": [\n",
    "        {\n",
    "            \"input\": inp, \n",
    "            \"true_output\": true, \n",
    "            \"pred_output\": pred,\n",
    "            \"true_direct\": extract_causes(true)[0],\n",
    "            \"true_indirect\": extract_causes(true)[1],\n",
    "            \"pred_direct\": extract_causes(pred)[0],\n",
    "            \"pred_indirect\": extract_causes(pred)[1]\n",
    "        }\n",
    "        for inp, true, pred in zip(test_inputs, true_outputs, pred_outputs)\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Detailed evaluation results saved to: {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic_env)",
   "language": "python",
   "name": "bertopic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

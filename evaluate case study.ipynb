{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773c07e-4c46-4fcc-b19f-8fc716554b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# é…ç½®è®¾ç½®\n",
    "base_model_name = \"unsloth/Qwen3-14B\"\n",
    "adapter_path = \"\"  # é€‚é…å™¨ç›®å½•\n",
    "csv_path = r\"\"  # æ›¿æ¢ä¸ºæ‚¨çš„CSVæ–‡ä»¶è·¯å¾„\n",
    "output_csv_path = r\"\"  # æ›¿æ¢ä¸ºè¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "batch_size = 1  # æ‰¹å¤„ç†å¤§å°\n",
    "\n",
    "\n",
    "# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "def load_model():\n",
    "    print(\"â³ åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "    # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹\n",
    "    base_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_name,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”„ åŠ è½½PEFTé€‚é…å™¨...\")\n",
    "    # ä½¿ç”¨æ ‡å‡†çš„PeftModelåŠ è½½é€‚é…å™¨\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        adapter_name=\"accident_cause_adapter\"\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"ğŸ”  åŠ è½½åˆ†è¯å™¨...\")\n",
    "    # ä»é€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        adapter_path,\n",
    "        padding_side=\"left\",\n",
    "        truncation_side=\"left\"\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 2. åˆ›å»ºæ¨ç†ç®¡é“ - ä¿®æ­£è®¾å¤‡é—®é¢˜\n",
    "def create_inference_pipeline(model, tokenizer):\n",
    "    print(\"âš™ï¸ åˆ›å»ºæ¨ç†ç®¡é“...\")\n",
    "    # ç§»é™¤deviceå‚æ•°ï¼Œå› ä¸ºæ¨¡å‹å·²é€šè¿‡device_mapè‡ªåŠ¨åˆ†é…åˆ°è®¾å¤‡\n",
    "    return pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# 3. ç”Ÿæˆå“åº”çš„å‡½æ•°\n",
    "def generate_response(pipe, input_text):\n",
    "    try:\n",
    "        # æ„å»ºAlpacaæ ¼å¼æç¤º\n",
    "        prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä¸ªäº‹æ•…å› æœæ¨ç†ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®inputä¸­çš„äº‹æ•…è¿‡ç¨‹ä»ç»™å‡ºçš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› åˆ†ç±»è¡¨ä¸­æ¨ç†å‡ºäº‹æ•…å¯¹åº”çš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› ï¼Œè¾“å‡ºåªèƒ½ä¸ºï¼šæ€ç»´é“¾ã€‚ç›´æ¥åŸå› :ç›´æ¥åŸå› åˆ—è¡¨ï¼Œé—´æ¥åŸå› :é—´æ¥åŸå› åˆ—è¡¨ã€‚ç›´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„,è¶…é€Ÿä¸è½½é‡åˆè§„æ€§,é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„,é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ,è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†,è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ,å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ.é—´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨,è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ,é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½,ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆã€‚\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        formatted_prompt = prompt_template.format(input_text)\n",
    "        \n",
    "        # ç”Ÿæˆå‚æ•°\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\":2048,  # å‡å°‘ç”Ÿæˆé•¿åº¦ä»¥èŠ‚çœå†…å­˜\n",
    "            \"temperature\": 0.6,    # é™ä½éšæœºæ€§\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 30,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": pipe.tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.2\n",
    "        }\n",
    "        \n",
    "        # ç”Ÿæˆå“åº”\n",
    "        outputs = pipe(\n",
    "            formatted_prompt,\n",
    "            **generation_config,\n",
    "            return_full_text=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # æå–å“åº”å†…å®¹\n",
    "        response = outputs[0]['generated_text'].strip()\n",
    "        \n",
    "        # æ¸…ç†å“åº”å†…å®¹\n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹\n",
    "        response = re.sub(r'(ç›´æ¥åŸå› |é—´æ¥åŸå› ).*?[:ï¼š]', '', response, count=1)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç”Ÿæˆé”™è¯¯: {str(e)}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# 4. ä¸»å¤„ç†æµç¨‹\n",
    "def process_csv(csv_path, output_path):\n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model, tokenizer = load_model()\n",
    "    pipe = create_inference_pipeline(model, tokenizer)\n",
    "    \n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    print(f\"ğŸ“– è¯»å–CSVæ–‡ä»¶: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–CSVå¤±è´¥: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # ç¡®ä¿å­˜åœ¨ç›®æ ‡åˆ—\n",
    "    if \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\" not in df.columns:\n",
    "        print(\"âŒ CSVæ–‡ä»¶ä¸­ç¼ºå°‘'äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°'åˆ—\")\n",
    "        return\n",
    "    \n",
    "    # æ·»åŠ ç»“æœåˆ—\n",
    "    df[\"æ¨¡å‹æ¨ç†ç»“æœ\"] = \"\"\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"ğŸš€ å¼€å§‹å¤„ç†äº‹æ•…æè¿°...\")\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡\n",
    "    progress_bar = tqdm(total=total_rows, desc=\"å¤„ç†äº‹æ•…æ•°æ®\", unit=\"è¡Œ\")\n",
    "    \n",
    "    for i in range(total_rows):\n",
    "        accident_desc = df.at[i, \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\"]\n",
    "        \n",
    "        # è·³è¿‡ç©ºæè¿°\n",
    "        if pd.isna(accident_desc) or str(accident_desc).strip() == \"\":\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = \"SKIPPED: ç©ºæè¿°\"\n",
    "            progress_bar.update(1)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # ç”Ÿæˆæ¨ç†ç»“æœ\n",
    "            result = generate_response(pipe, str(accident_desc))\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = result\n",
    "            processed_count += 1\n",
    "            \n",
    "            # æ¯å¤„ç†5è¡Œä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ\n",
    "            if processed_count > 0 and processed_count % 5 == 0:\n",
    "                df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ ({processed_count}/{total_rows})\")\n",
    "            \n",
    "            # æ¯å¤„ç†10è¡Œæ¸…ç†ä¸€æ¬¡å†…å­˜\n",
    "            if processed_count > 0 and processed_count % 10 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(\"ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¡Œ {i+1} å¤„ç†å¤±è´¥: {str(e)}\")\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = f\"ERROR: {str(e)}\"\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… å¤„ç†å®Œæˆ! å…±å¤„ç† {processed_count} æ¡è®°å½•ï¼Œç»“æœä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    # é‡Šæ”¾èµ„æº\n",
    "    del model, tokenizer, pipe\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# æ‰§è¡Œå¤„ç†\n",
    "if __name__ == \"__main__\":\n",
    "    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(f\"âŒ é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {adapter_path}\")\n",
    "    else:\n",
    "        print(f\"ğŸ” é€‚é…å™¨è·¯å¾„éªŒè¯: {adapter_path} (åŒ…å« {len(os.listdir(adapter_path))} ä¸ªæ–‡ä»¶)\")\n",
    "    \n",
    "    # è®¾ç½®ç¯å¢ƒå˜é‡é˜²æ­¢å¹¶è¡Œå¤„ç†å†²çª\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # è®¾ç½®CUDAè®¾å¤‡å¯è§æ€§\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU\n",
    "    \n",
    "    process_csv(csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7d10d-abe3-4f33-af68-cf614139ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# å®šä¹‰åŸå› åˆ†ç±»è¡¨\n",
    "DIRECT_CAUSES = [\n",
    "    \"è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„\",\n",
    "    \"è¶…é€Ÿä¸è½½é‡åˆè§„æ€§\",\n",
    "    \"é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„\",\n",
    "    \"é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ\",\n",
    "    \"è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†\",\n",
    "    \"è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ\",\n",
    "    \"å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ\"\n",
    "]\n",
    "\n",
    "INDIRECT_CAUSES = [\n",
    "    \"æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨\",\n",
    "    \"è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ\",\n",
    "    \"é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½\",\n",
    "    \"ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆ\"\n",
    "]\n",
    "\n",
    "class RobustCauseParser:\n",
    "    \"\"\"é²æ£’çš„åŸå› è§£æå™¨ï¼Œå¤„ç†ä¸åŒæ ¼å¼çš„åŸå› æè¿°\"\"\"\n",
    "    \n",
    "    def extract_causes(self, text):\n",
    "        \"\"\"ä»æ–‡æœ¬ä¸­æå–åŸå› åˆ—è¡¨\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return [], []\n",
    "            \n",
    "        # å°è¯•ä»æ–‡æœ¬ä¸­æå–åŸå› \n",
    "        direct_list = []\n",
    "        indirect_list = []\n",
    "        \n",
    "        # å°è¯•åŒ¹é…æ ¼å¼åŒ–çš„åŸå› åˆ—è¡¨\n",
    "        if \"ç›´æ¥åŸå› :\" in text and \"é—´æ¥åŸå› :\" in text:\n",
    "            try:\n",
    "                # å°è¯•æå–ç›´æ¥åŸå› éƒ¨åˆ†\n",
    "                direct_part = re.search(r\"ç›´æ¥åŸå› [:ï¼š]\\s*(.+?)(é—´æ¥åŸå› |$)\", text, re.DOTALL)\n",
    "                if direct_part:\n",
    "                    direct_str = direct_part.group(1).strip()\n",
    "                    direct_list = [c.strip() for c in re.split(r\"[,ï¼Œ]\", direct_str) if c.strip()]\n",
    "                \n",
    "                # å°è¯•æå–é—´æ¥åŸå› éƒ¨åˆ†\n",
    "                indirect_part = re.search(r\"é—´æ¥åŸå› [:ï¼š]\\s*(.+)\", text)\n",
    "                if indirect_part:\n",
    "                    indirect_str = indirect_part.group(1).strip()\n",
    "                    indirect_list = [c.strip() for c in re.split(r\"[,ï¼Œ]\", indirect_str) if c.strip()]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ¼å¼åŒ–çš„åŸå› ï¼Œå°è¯•ä»è‡ªç”±æ–‡æœ¬ä¸­åŒ¹é…\n",
    "        if not direct_list:\n",
    "            for cause in DIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    direct_list.append(cause)\n",
    "        \n",
    "        if not indirect_list:\n",
    "            for cause in INDIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    indirect_list.append(cause)\n",
    "        \n",
    "        return direct_list, indirect_list\n",
    "\n",
    "class CauseMapper:\n",
    "    \"\"\"å°†åŸå§‹åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»è¡¨\"\"\"\n",
    "    \n",
    "    def __init__(self, direct_causes, indirect_causes):\n",
    "        self.direct_causes = direct_causes\n",
    "        self.indirect_causes = indirect_causes\n",
    "        \n",
    "    def map_direct(self, causes):\n",
    "        \"\"\"å°†åŸå§‹ç›´æ¥åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            # å°è¯•å®Œå…¨åŒ¹é…\n",
    "            if cause in self.direct_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            # å°è¯•éƒ¨åˆ†åŒ¹é…\n",
    "            for standard_cause in self.direct_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))  # å»é‡\n",
    "    \n",
    "    def map_indirect(self, causes):\n",
    "        \"\"\"å°†åŸå§‹é—´æ¥åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            if cause in self.indirect_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            for standard_cause in self.indirect_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))\n",
    "\n",
    "def calculate_classification_metrics(true_labels, pred_labels, class_list):\n",
    "    \"\"\"è®¡ç®—åˆ†ç±»æŒ‡æ ‡ï¼ˆå®å¹³å‡ï¼‰\"\"\"\n",
    "    # å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡\n",
    "    true_vectors = []\n",
    "    pred_vectors = []\n",
    "    \n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_vec = [1 if c in true else 0 for c in class_list]\n",
    "        pred_vec = [1 if c in pred else 0 for c in class_list]\n",
    "        true_vectors.append(true_vec)\n",
    "        pred_vectors.append(pred_vec)\n",
    "    \n",
    "    # å±•å¹³å‘é‡ç”¨äºæ•´ä½“æŒ‡æ ‡è®¡ç®—\n",
    "    flat_true = np.array(true_vectors).flatten()\n",
    "    flat_pred = np.array(pred_vectors).flatten()\n",
    "    \n",
    "    # è®¡ç®—æ•´ä½“æŒ‡æ ‡\n",
    "    precision = precision_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡\n",
    "    class_metrics = {}\n",
    "    for i, cause in enumerate(class_list):\n",
    "        class_true = [vec[i] for vec in true_vectors]\n",
    "        class_pred = [vec[i] for vec in pred_vectors]\n",
    "        \n",
    "        class_metrics[cause] = {\n",
    "            \"precision\": precision_score(class_true, class_pred, zero_division=0),\n",
    "            \"recall\": recall_score(class_true, class_pred, zero_division=0),\n",
    "            \"f1\": f1_score(class_true, class_pred, zero_division=0),\n",
    "            \"support\": sum(class_true)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"overall\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        },\n",
    "        \"per_class\": class_metrics\n",
    "    }\n",
    "\n",
    "def evaluate_csv(csv_path):\n",
    "    \"\"\"è¯„ä¼°CSVæ–‡ä»¶ä¸­çš„é¢„æµ‹ç»“æœ\"\"\"\n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    print(f\"ğŸ“– è¯»å–CSVæ–‡ä»¶: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–CSVå¤±è´¥: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    required_columns = ['ç›´æ¥åŸå› ', 'é—´æ¥åŸå› ', 'åŒ¹é…çš„ç›´æ¥åŸå› ', 'åŒ¹é…çš„é—´æ¥åŸå› ']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"âŒ CSVæ–‡ä»¶ä¸­ç¼ºå°‘'{col}'åˆ—\")\n",
    "            return None\n",
    "    \n",
    "    # åˆå§‹åŒ–å·¥å…·\n",
    "    parser = RobustCauseParser()\n",
    "    mapper = CauseMapper(DIRECT_CAUSES, INDIRECT_CAUSES)\n",
    "    \n",
    "    # å‡†å¤‡è¯„ä¼°æ•°æ®å®¹å™¨\n",
    "    evaluation_data = {\n",
    "        \"direct\": {\"true\": [], \"pred\": []},\n",
    "        \"indirect\": {\"true\": [], \"pred\": []},\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ” è§£æåŸå› å¹¶æ˜ å°„åˆ°åˆ†ç±»è¡¨...\")\n",
    "    \n",
    "    # éå†æ¯ä¸€è¡Œ\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"å¤„ç†è¡Œ\"):\n",
    "        # è§£æçœŸå®åŸå› \n",
    "        true_direct_raw = str(row[\"ç›´æ¥åŸå› \"]) if not pd.isna(row[\"ç›´æ¥åŸå› \"]) else \"\"\n",
    "        true_indirect_raw = str(row[\"é—´æ¥åŸå› \"]) if not pd.isna(row[\"é—´æ¥åŸå› \"]) else \"\"\n",
    "        \n",
    "        # è§£æé¢„æµ‹åŸå› \n",
    "        pred_direct_raw = str(row[\"åŒ¹é…çš„ç›´æ¥åŸå› \"]) if not pd.isna(row[\"åŒ¹é…çš„ç›´æ¥åŸå› \"]) else \"\"\n",
    "        pred_indirect_raw = str(row[\"åŒ¹é…çš„é—´æ¥åŸå› \"]) if not pd.isna(row[\"åŒ¹é…çš„é—´æ¥åŸå› \"]) else \"\"\n",
    "        \n",
    "        # æ˜ å°„åˆ°åˆ†ç±»è¡¨\n",
    "        true_direct_mapped = mapper.map_direct(parser.extract_causes(true_direct_raw)[0])\n",
    "        true_indirect_mapped = mapper.map_indirect(parser.extract_causes(true_indirect_raw)[1])\n",
    "        pred_direct_mapped = mapper.map_direct(parser.extract_causes(pred_direct_raw)[0])\n",
    "        pred_indirect_mapped = mapper.map_indirect(parser.extract_causes(pred_indirect_raw)[1])\n",
    "        \n",
    "        # å­˜å‚¨ç»“æœ\n",
    "        evaluation_data[\"direct\"][\"true\"].append(true_direct_mapped)\n",
    "        evaluation_data[\"direct\"][\"pred\"].append(pred_direct_mapped)\n",
    "        evaluation_data[\"indirect\"][\"true\"].append(true_indirect_mapped)\n",
    "        evaluation_data[\"indirect\"][\"pred\"].append(pred_indirect_mapped)\n",
    "        \n",
    "        # ä¿å­˜è¯¦ç»†ç»“æœç”¨äºåˆ†æ\n",
    "        evaluation_data[\"details\"].append({\n",
    "            \"index\": i,\n",
    "            \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\": row[\"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\"] if \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\" in row else \"\",\n",
    "            \"true_direct_raw\": true_direct_raw,\n",
    "            \"true_indirect_raw\": true_indirect_raw,\n",
    "            \"pred_direct_raw\": pred_direct_raw,\n",
    "            \"pred_indirect_raw\": pred_indirect_raw,\n",
    "            \"true_direct_mapped\": true_direct_mapped,\n",
    "            \"true_indirect_mapped\": true_indirect_mapped,\n",
    "            \"pred_direct_mapped\": pred_direct_mapped,\n",
    "            \"pred_indirect_mapped\": pred_indirect_mapped\n",
    "        })\n",
    "    \n",
    "    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡\n",
    "    print(\"ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...\")\n",
    "    results = {\n",
    "        \"direct\": calculate_classification_metrics(\n",
    "            evaluation_data[\"direct\"][\"true\"], \n",
    "            evaluation_data[\"direct\"][\"pred\"], \n",
    "            DIRECT_CAUSES\n",
    "        ),\n",
    "        \"indirect\": calculate_classification_metrics(\n",
    "            evaluation_data[\"indirect\"][\"true\"], \n",
    "            evaluation_data[\"indirect\"][\"pred\"], \n",
    "            INDIRECT_CAUSES\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\n===== è¯„ä¼°ç»“æœ =====\")\n",
    "    print(f\"ç›´æ¥åŸå› ç²¾ç¡®ç‡: {results['direct']['overall']['precision']:.4f}\")\n",
    "    print(f\"ç›´æ¥åŸå› å¬å›ç‡: {results['direct']['overall']['recall']:.4f}\")\n",
    "    print(f\"ç›´æ¥åŸå› F1: {results['direct']['overall']['f1']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› ç²¾ç¡®ç‡: {results['indirect']['overall']['precision']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› å¬å›ç‡: {results['indirect']['overall']['recall']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› F1: {results['indirect']['overall']['f1']:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´ç»“æœ\n",
    "    output_path = csv_path.replace(\".csv\", \"_evaluation_results.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"metrics\": results,\n",
    "            \"details\": evaluation_data[\"details\"]\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… è¯„ä¼°å®Œæˆ! è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "if __name__ == \"__main__\":\n",
    "    # CSVæ–‡ä»¶è·¯å¾„\n",
    "    csv_path = r\"\"\n",
    "    \n",
    "    # è¿è¡Œè¯„ä¼°\n",
    "    results = evaluate_csv(csv_path)\n",
    "    \n",
    "    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡\n",
    "    if results:\n",
    "        print(\"\\n===== ç›´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\")\n",
    "        for cause, metrics in results[\"direct\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}\")\n",
    "            print(f\"  å¬å›ç‡: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  æ”¯æŒåº¦: {metrics['support']}\")\n",
    "        \n",
    "        print(\"\\n===== é—´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\")\n",
    "        for cause, metrics in results[\"indirect\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}\")\n",
    "            print(f\"  å¬å›ç‡: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  æ”¯æŒåº¦: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c8f33-68e7-451b-a22e-cf0087076bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

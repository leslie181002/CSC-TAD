{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2773c07e-4c46-4fcc-b19f-8fc716554b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "ğŸ” é€‚é…å™¨è·¯å¾„éªŒè¯: D:/ft/qwen-14b-1000-new/fine_tuned_model (åŒ…å« 9 ä¸ªæ–‡ä»¶)\n",
      "â³ åŠ è½½åŸºç¡€æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeslieHailee\\anaconda3\\envs\\finetuning\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.842 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a791c41eb37b409c82ef673205bf2243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ åŠ è½½PEFTé€‚é…å™¨...\n",
      "ğŸ”  åŠ è½½åˆ†è¯å™¨...\n",
      "âš™ï¸ åˆ›å»ºæ¨ç†ç®¡é“...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¯»å–CSVæ–‡ä»¶: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents.csv\n",
      "ğŸš€ å¼€å§‹å¤„ç†äº‹æ•…æè¿°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:   6%|â–Œ         | 5/82 [01:29<21:53, 17.06s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (5/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  11%|â–ˆ         | 9/82 [02:36<20:03, 16.49s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (10/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  12%|â–ˆâ–        | 10/82 [02:51<19:03, 15.88s/è¡Œ]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  18%|â–ˆâ–Š        | 15/82 [04:25<20:39, 18.49s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (15/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  23%|â–ˆâ–ˆâ–       | 19/82 [05:46<22:24, 21.35s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (20/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  24%|â–ˆâ–ˆâ–       | 20/82 [06:04<21:12, 20.53s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  30%|â–ˆâ–ˆâ–ˆ       | 25/82 [07:36<18:38, 19.63s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (25/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 29/82 [08:39<14:34, 16.50s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (30/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 30/82 [09:02<15:59, 18.45s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 35/82 [10:35<14:46, 18.85s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (35/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 39/82 [11:57<14:08, 19.74s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (40/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 40/82 [12:13<13:04, 18.67s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 45/82 [13:38<10:41, 17.33s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (45/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 49/82 [14:54<10:11, 18.53s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (50/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 50/82 [15:12<09:53, 18.56s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 55/82 [16:42<07:42, 17.15s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (55/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/82 [18:08<07:52, 20.53s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (60/82)\n",
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 65/82 [19:51<04:51, 17.15s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (65/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 69/82 [21:10<03:56, 18.22s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (70/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 70/82 [21:24<03:23, 16.95s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 75/82 [22:53<02:00, 17.16s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (75/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 79/82 [24:10<00:53, 17.94s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ (80/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†äº‹æ•…æ•°æ®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [24:30<00:00, 17.93s/è¡Œ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n",
      "âœ… å¤„ç†å®Œæˆ! å…±å¤„ç† 80 æ¡è®°å½•ï¼Œç»“æœä¿å­˜è‡³: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# é…ç½®è®¾ç½®\n",
    "base_model_name = \"unsloth/Qwen3-14B\"\n",
    "adapter_path = \"D:/ft/qwen-14b-1000-new/fine_tuned_model\"  # é€‚é…å™¨ç›®å½•\n",
    "csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents.csv\"  # æ›¿æ¢ä¸ºæ‚¨çš„CSVæ–‡ä»¶è·¯å¾„\n",
    "output_csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents_data.csv\"  # æ›¿æ¢ä¸ºè¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "batch_size = 1  # æ‰¹å¤„ç†å¤§å°\n",
    "\n",
    "\n",
    "# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "def load_model():\n",
    "    print(\"â³ åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "    # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹\n",
    "    base_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_name,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”„ åŠ è½½PEFTé€‚é…å™¨...\")\n",
    "    # ä½¿ç”¨æ ‡å‡†çš„PeftModelåŠ è½½é€‚é…å™¨\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        adapter_name=\"accident_cause_adapter\"\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"ğŸ”  åŠ è½½åˆ†è¯å™¨...\")\n",
    "    # ä»é€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        adapter_path,\n",
    "        padding_side=\"left\",\n",
    "        truncation_side=\"left\"\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 2. åˆ›å»ºæ¨ç†ç®¡é“ - ä¿®æ­£è®¾å¤‡é—®é¢˜\n",
    "def create_inference_pipeline(model, tokenizer):\n",
    "    print(\"âš™ï¸ åˆ›å»ºæ¨ç†ç®¡é“...\")\n",
    "    # ç§»é™¤deviceå‚æ•°ï¼Œå› ä¸ºæ¨¡å‹å·²é€šè¿‡device_mapè‡ªåŠ¨åˆ†é…åˆ°è®¾å¤‡\n",
    "    return pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# 3. ç”Ÿæˆå“åº”çš„å‡½æ•°\n",
    "def generate_response(pipe, input_text):\n",
    "    try:\n",
    "        # æ„å»ºAlpacaæ ¼å¼æç¤º\n",
    "        prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä¸ªäº‹æ•…å› æœæ¨ç†ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®inputä¸­çš„äº‹æ•…è¿‡ç¨‹ä»ç»™å‡ºçš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› åˆ†ç±»è¡¨ä¸­æ¨ç†å‡ºäº‹æ•…å¯¹åº”çš„ç›´æ¥åŸå› å’Œé—´æ¥åŸå› ï¼Œè¾“å‡ºåªèƒ½ä¸ºï¼šæ€ç»´é“¾ã€‚ç›´æ¥åŸå› :ç›´æ¥åŸå› åˆ—è¡¨ï¼Œé—´æ¥åŸå› :é—´æ¥åŸå› åˆ—è¡¨ã€‚ç›´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„,è¶…é€Ÿä¸è½½é‡åˆè§„æ€§,é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„,é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ,è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†,è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ,å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ.é—´æ¥åŸå› åˆ†ç±»è¡¨æœ‰:æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨,è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ,é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½,ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆã€‚\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        formatted_prompt = prompt_template.format(input_text)\n",
    "        \n",
    "        # ç”Ÿæˆå‚æ•°\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\":2048,  # å‡å°‘ç”Ÿæˆé•¿åº¦ä»¥èŠ‚çœå†…å­˜\n",
    "            \"temperature\": 0.6,    # é™ä½éšæœºæ€§\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 30,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": pipe.tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.2\n",
    "        }\n",
    "        \n",
    "        # ç”Ÿæˆå“åº”\n",
    "        outputs = pipe(\n",
    "            formatted_prompt,\n",
    "            **generation_config,\n",
    "            return_full_text=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # æå–å“åº”å†…å®¹\n",
    "        response = outputs[0]['generated_text'].strip()\n",
    "        \n",
    "        # æ¸…ç†å“åº”å†…å®¹\n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹\n",
    "        response = re.sub(r'(ç›´æ¥åŸå› |é—´æ¥åŸå› ).*?[:ï¼š]', '', response, count=1)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç”Ÿæˆé”™è¯¯: {str(e)}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# 4. ä¸»å¤„ç†æµç¨‹\n",
    "def process_csv(csv_path, output_path):\n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model, tokenizer = load_model()\n",
    "    pipe = create_inference_pipeline(model, tokenizer)\n",
    "    \n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    print(f\"ğŸ“– è¯»å–CSVæ–‡ä»¶: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–CSVå¤±è´¥: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # ç¡®ä¿å­˜åœ¨ç›®æ ‡åˆ—\n",
    "    if \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\" not in df.columns:\n",
    "        print(\"âŒ CSVæ–‡ä»¶ä¸­ç¼ºå°‘'äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°'åˆ—\")\n",
    "        return\n",
    "    \n",
    "    # æ·»åŠ ç»“æœåˆ—\n",
    "    df[\"æ¨¡å‹æ¨ç†ç»“æœ\"] = \"\"\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"ğŸš€ å¼€å§‹å¤„ç†äº‹æ•…æè¿°...\")\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡\n",
    "    progress_bar = tqdm(total=total_rows, desc=\"å¤„ç†äº‹æ•…æ•°æ®\", unit=\"è¡Œ\")\n",
    "    \n",
    "    for i in range(total_rows):\n",
    "        accident_desc = df.at[i, \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\"]\n",
    "        \n",
    "        # è·³è¿‡ç©ºæè¿°\n",
    "        if pd.isna(accident_desc) or str(accident_desc).strip() == \"\":\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = \"SKIPPED: ç©ºæè¿°\"\n",
    "            progress_bar.update(1)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # ç”Ÿæˆæ¨ç†ç»“æœ\n",
    "            result = generate_response(pipe, str(accident_desc))\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = result\n",
    "            processed_count += 1\n",
    "            \n",
    "            # æ¯å¤„ç†5è¡Œä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ\n",
    "            if processed_count > 0 and processed_count % 5 == 0:\n",
    "                df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ ({processed_count}/{total_rows})\")\n",
    "            \n",
    "            # æ¯å¤„ç†10è¡Œæ¸…ç†ä¸€æ¬¡å†…å­˜\n",
    "            if processed_count > 0 and processed_count % 10 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(\"ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¡Œ {i+1} å¤„ç†å¤±è´¥: {str(e)}\")\n",
    "            df.at[i, \"æ¨¡å‹æ¨ç†ç»“æœ\"] = f\"ERROR: {str(e)}\"\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… å¤„ç†å®Œæˆ! å…±å¤„ç† {processed_count} æ¡è®°å½•ï¼Œç»“æœä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    # é‡Šæ”¾èµ„æº\n",
    "    del model, tokenizer, pipe\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# æ‰§è¡Œå¤„ç†\n",
    "if __name__ == \"__main__\":\n",
    "    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(f\"âŒ é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {adapter_path}\")\n",
    "    else:\n",
    "        print(f\"ğŸ” é€‚é…å™¨è·¯å¾„éªŒè¯: {adapter_path} (åŒ…å« {len(os.listdir(adapter_path))} ä¸ªæ–‡ä»¶)\")\n",
    "    \n",
    "    # è®¾ç½®ç¯å¢ƒå˜é‡é˜²æ­¢å¹¶è¡Œå¤„ç†å†²çª\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # è®¾ç½®CUDAè®¾å¤‡å¯è§æ€§\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU\n",
    "    \n",
    "    process_csv(csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff7d10d-abe3-4f33-af68-cf614139ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¯»å–CSVæ–‡ä»¶: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents_data_with_causes.csv\n",
      "ğŸ” è§£æåŸå› å¹¶æ˜ å°„åˆ°åˆ†ç±»è¡¨...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†è¡Œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:00<00:00, 40998.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...\n",
      "\n",
      "===== è¯„ä¼°ç»“æœ =====\n",
      "ç›´æ¥åŸå› ç²¾ç¡®ç‡: 0.6992\n",
      "ç›´æ¥åŸå› å¬å›ç‡: 0.6509\n",
      "ç›´æ¥åŸå› F1: 0.6669\n",
      "é—´æ¥åŸå› ç²¾ç¡®ç‡: 0.6389\n",
      "é—´æ¥åŸå› å¬å›ç‡: 0.6065\n",
      "é—´æ¥åŸå› F1: 0.6028\n",
      "\n",
      "âœ… è¯„ä¼°å®Œæˆ! è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents_data_with_causes_evaluation_results.json\n",
      "\n",
      "===== ç›´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\n",
      "è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„:\n",
      "  ç²¾ç¡®ç‡: 0.5000\n",
      "  å¬å›ç‡: 0.1875\n",
      "  F1: 0.2727\n",
      "  æ”¯æŒåº¦: 16\n",
      "è¶…é€Ÿä¸è½½é‡åˆè§„æ€§:\n",
      "  ç²¾ç¡®ç‡: 0.5000\n",
      "  å¬å›ç‡: 0.5833\n",
      "  F1: 0.5385\n",
      "  æ”¯æŒåº¦: 12\n",
      "é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„:\n",
      "  ç²¾ç¡®ç‡: 0.8333\n",
      "  å¬å›ç‡: 0.2778\n",
      "  F1: 0.4167\n",
      "  æ”¯æŒåº¦: 18\n",
      "é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ:\n",
      "  ç²¾ç¡®ç‡: 0.5556\n",
      "  å¬å›ç‡: 0.2174\n",
      "  F1: 0.3125\n",
      "  æ”¯æŒåº¦: 23\n",
      "è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†:\n",
      "  ç²¾ç¡®ç‡: 0.5000\n",
      "  å¬å›ç‡: 0.3000\n",
      "  F1: 0.3750\n",
      "  æ”¯æŒåº¦: 20\n",
      "è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ:\n",
      "  ç²¾ç¡®ç‡: 0.7000\n",
      "  å¬å›ç‡: 0.3889\n",
      "  F1: 0.5000\n",
      "  æ”¯æŒåº¦: 18\n",
      "å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ:\n",
      "  ç²¾ç¡®ç‡: 0.5161\n",
      "  å¬å›ç‡: 0.8421\n",
      "  F1: 0.6400\n",
      "  æ”¯æŒåº¦: 19\n",
      "\n",
      "===== é—´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\n",
      "æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨:\n",
      "  ç²¾ç¡®ç‡: 0.6000\n",
      "  å¬å›ç‡: 0.4737\n",
      "  F1: 0.5294\n",
      "  æ”¯æŒåº¦: 38\n",
      "è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ:\n",
      "  ç²¾ç¡®ç‡: 0.2857\n",
      "  å¬å›ç‡: 0.0870\n",
      "  F1: 0.1333\n",
      "  æ”¯æŒåº¦: 23\n",
      "é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½:\n",
      "  ç²¾ç¡®ç‡: 0.8000\n",
      "  å¬å›ç‡: 0.1250\n",
      "  F1: 0.2162\n",
      "  æ”¯æŒåº¦: 32\n",
      "ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆ:\n",
      "  ç²¾ç¡®ç‡: 0.6579\n",
      "  å¬å›ç‡: 0.6410\n",
      "  F1: 0.6494\n",
      "  æ”¯æŒåº¦: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# å®šä¹‰åŸå› åˆ†ç±»è¡¨\n",
    "DIRECT_CAUSES = [\n",
    "    \"è½¦è¾†æ“ä½œå®‰å…¨æ„è¯†æ·¡è–„\",\n",
    "    \"è¶…é€Ÿä¸è½½é‡åˆè§„æ€§\",\n",
    "    \"é«˜é£é™©è·¯æ®µé©¾é©¶è¡Œä¸ºä¸åˆè§„\",\n",
    "    \"é‡å‹è½¦è¾†æ“ä½œè§„èŒƒ\",\n",
    "    \"è½¦è¾†è¡Œé©¶ç¨³å®šæ€§ç®¡ç†\",\n",
    "    \"è½¦é—´è£…å¸è´§ç‰©è§„èŒƒå®‰å…¨æ“ä½œ\",\n",
    "    \"å·¥åœ°å¸è½½è´§ç‰©æ“ä½œä¸è§„èŒƒ\"\n",
    "]\n",
    "\n",
    "INDIRECT_CAUSES = [\n",
    "    \"æ–½å·¥ç°åœºäººå‘˜å®‰å…¨ç®¡ç†ä½“ç³»ä¸å¥å…¨\",\n",
    "    \"è¿è¾“è¿‡ç¨‹åˆè§„æ€§ç›‘ç®¡å¤±æ•ˆ\",\n",
    "    \"é‡å‹è½¦è¾†é©¾é©¶å‘˜è¿è¾“åŸ¹è®­ä¸åˆ°ä½\",\n",
    "    \"ä¼ä¸šç®¡ç†è½¦è¾†é©¾é©¶è§„èŒƒè´£ä»»å¤±æ•ˆ\"\n",
    "]\n",
    "\n",
    "class RobustCauseParser:\n",
    "    \"\"\"é²æ£’çš„åŸå› è§£æå™¨ï¼Œå¤„ç†ä¸åŒæ ¼å¼çš„åŸå› æè¿°\"\"\"\n",
    "    \n",
    "    def extract_causes(self, text):\n",
    "        \"\"\"ä»æ–‡æœ¬ä¸­æå–åŸå› åˆ—è¡¨\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return [], []\n",
    "            \n",
    "        # å°è¯•ä»æ–‡æœ¬ä¸­æå–åŸå› \n",
    "        direct_list = []\n",
    "        indirect_list = []\n",
    "        \n",
    "        # å°è¯•åŒ¹é…æ ¼å¼åŒ–çš„åŸå› åˆ—è¡¨\n",
    "        if \"ç›´æ¥åŸå› :\" in text and \"é—´æ¥åŸå› :\" in text:\n",
    "            try:\n",
    "                # å°è¯•æå–ç›´æ¥åŸå› éƒ¨åˆ†\n",
    "                direct_part = re.search(r\"ç›´æ¥åŸå› [:ï¼š]\\s*(.+?)(é—´æ¥åŸå› |$)\", text, re.DOTALL)\n",
    "                if direct_part:\n",
    "                    direct_str = direct_part.group(1).strip()\n",
    "                    direct_list = [c.strip() for c in re.split(r\"[,ï¼Œ]\", direct_str) if c.strip()]\n",
    "                \n",
    "                # å°è¯•æå–é—´æ¥åŸå› éƒ¨åˆ†\n",
    "                indirect_part = re.search(r\"é—´æ¥åŸå› [:ï¼š]\\s*(.+)\", text)\n",
    "                if indirect_part:\n",
    "                    indirect_str = indirect_part.group(1).strip()\n",
    "                    indirect_list = [c.strip() for c in re.split(r\"[,ï¼Œ]\", indirect_str) if c.strip()]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ¼å¼åŒ–çš„åŸå› ï¼Œå°è¯•ä»è‡ªç”±æ–‡æœ¬ä¸­åŒ¹é…\n",
    "        if not direct_list:\n",
    "            for cause in DIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    direct_list.append(cause)\n",
    "        \n",
    "        if not indirect_list:\n",
    "            for cause in INDIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    indirect_list.append(cause)\n",
    "        \n",
    "        return direct_list, indirect_list\n",
    "\n",
    "class CauseMapper:\n",
    "    \"\"\"å°†åŸå§‹åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»è¡¨\"\"\"\n",
    "    \n",
    "    def __init__(self, direct_causes, indirect_causes):\n",
    "        self.direct_causes = direct_causes\n",
    "        self.indirect_causes = indirect_causes\n",
    "        \n",
    "    def map_direct(self, causes):\n",
    "        \"\"\"å°†åŸå§‹ç›´æ¥åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            # å°è¯•å®Œå…¨åŒ¹é…\n",
    "            if cause in self.direct_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            # å°è¯•éƒ¨åˆ†åŒ¹é…\n",
    "            for standard_cause in self.direct_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))  # å»é‡\n",
    "    \n",
    "    def map_indirect(self, causes):\n",
    "        \"\"\"å°†åŸå§‹é—´æ¥åŸå› æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            if cause in self.indirect_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            for standard_cause in self.indirect_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))\n",
    "\n",
    "def calculate_classification_metrics(true_labels, pred_labels, class_list):\n",
    "    \"\"\"è®¡ç®—åˆ†ç±»æŒ‡æ ‡ï¼ˆå®å¹³å‡ï¼‰\"\"\"\n",
    "    # å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡\n",
    "    true_vectors = []\n",
    "    pred_vectors = []\n",
    "    \n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_vec = [1 if c in true else 0 for c in class_list]\n",
    "        pred_vec = [1 if c in pred else 0 for c in class_list]\n",
    "        true_vectors.append(true_vec)\n",
    "        pred_vectors.append(pred_vec)\n",
    "    \n",
    "    # å±•å¹³å‘é‡ç”¨äºæ•´ä½“æŒ‡æ ‡è®¡ç®—\n",
    "    flat_true = np.array(true_vectors).flatten()\n",
    "    flat_pred = np.array(pred_vectors).flatten()\n",
    "    \n",
    "    # è®¡ç®—æ•´ä½“æŒ‡æ ‡\n",
    "    precision = precision_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡\n",
    "    class_metrics = {}\n",
    "    for i, cause in enumerate(class_list):\n",
    "        class_true = [vec[i] for vec in true_vectors]\n",
    "        class_pred = [vec[i] for vec in pred_vectors]\n",
    "        \n",
    "        class_metrics[cause] = {\n",
    "            \"precision\": precision_score(class_true, class_pred, zero_division=0),\n",
    "            \"recall\": recall_score(class_true, class_pred, zero_division=0),\n",
    "            \"f1\": f1_score(class_true, class_pred, zero_division=0),\n",
    "            \"support\": sum(class_true)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"overall\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        },\n",
    "        \"per_class\": class_metrics\n",
    "    }\n",
    "\n",
    "def evaluate_csv(csv_path):\n",
    "    \"\"\"è¯„ä¼°CSVæ–‡ä»¶ä¸­çš„é¢„æµ‹ç»“æœ\"\"\"\n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    print(f\"ğŸ“– è¯»å–CSVæ–‡ä»¶: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–CSVå¤±è´¥: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    required_columns = ['ç›´æ¥åŸå› ', 'é—´æ¥åŸå› ', 'åŒ¹é…çš„ç›´æ¥åŸå› ', 'åŒ¹é…çš„é—´æ¥åŸå› ']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"âŒ CSVæ–‡ä»¶ä¸­ç¼ºå°‘'{col}'åˆ—\")\n",
    "            return None\n",
    "    \n",
    "    # åˆå§‹åŒ–å·¥å…·\n",
    "    parser = RobustCauseParser()\n",
    "    mapper = CauseMapper(DIRECT_CAUSES, INDIRECT_CAUSES)\n",
    "    \n",
    "    # å‡†å¤‡è¯„ä¼°æ•°æ®å®¹å™¨\n",
    "    evaluation_data = {\n",
    "        \"direct\": {\"true\": [], \"pred\": []},\n",
    "        \"indirect\": {\"true\": [], \"pred\": []},\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ” è§£æåŸå› å¹¶æ˜ å°„åˆ°åˆ†ç±»è¡¨...\")\n",
    "    \n",
    "    # éå†æ¯ä¸€è¡Œ\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"å¤„ç†è¡Œ\"):\n",
    "        # è§£æçœŸå®åŸå› \n",
    "        true_direct_raw = str(row[\"ç›´æ¥åŸå› \"]) if not pd.isna(row[\"ç›´æ¥åŸå› \"]) else \"\"\n",
    "        true_indirect_raw = str(row[\"é—´æ¥åŸå› \"]) if not pd.isna(row[\"é—´æ¥åŸå› \"]) else \"\"\n",
    "        \n",
    "        # è§£æé¢„æµ‹åŸå› \n",
    "        pred_direct_raw = str(row[\"åŒ¹é…çš„ç›´æ¥åŸå› \"]) if not pd.isna(row[\"åŒ¹é…çš„ç›´æ¥åŸå› \"]) else \"\"\n",
    "        pred_indirect_raw = str(row[\"åŒ¹é…çš„é—´æ¥åŸå› \"]) if not pd.isna(row[\"åŒ¹é…çš„é—´æ¥åŸå› \"]) else \"\"\n",
    "        \n",
    "        # æ˜ å°„åˆ°åˆ†ç±»è¡¨\n",
    "        true_direct_mapped = mapper.map_direct(parser.extract_causes(true_direct_raw)[0])\n",
    "        true_indirect_mapped = mapper.map_indirect(parser.extract_causes(true_indirect_raw)[1])\n",
    "        pred_direct_mapped = mapper.map_direct(parser.extract_causes(pred_direct_raw)[0])\n",
    "        pred_indirect_mapped = mapper.map_indirect(parser.extract_causes(pred_indirect_raw)[1])\n",
    "        \n",
    "        # å­˜å‚¨ç»“æœ\n",
    "        evaluation_data[\"direct\"][\"true\"].append(true_direct_mapped)\n",
    "        evaluation_data[\"direct\"][\"pred\"].append(pred_direct_mapped)\n",
    "        evaluation_data[\"indirect\"][\"true\"].append(true_indirect_mapped)\n",
    "        evaluation_data[\"indirect\"][\"pred\"].append(pred_indirect_mapped)\n",
    "        \n",
    "        # ä¿å­˜è¯¦ç»†ç»“æœç”¨äºåˆ†æ\n",
    "        evaluation_data[\"details\"].append({\n",
    "            \"index\": i,\n",
    "            \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\": row[\"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\"] if \"äº‹æ•…ä¸­æ–­è¿‡ç¨‹æè¿°\" in row else \"\",\n",
    "            \"true_direct_raw\": true_direct_raw,\n",
    "            \"true_indirect_raw\": true_indirect_raw,\n",
    "            \"pred_direct_raw\": pred_direct_raw,\n",
    "            \"pred_indirect_raw\": pred_indirect_raw,\n",
    "            \"true_direct_mapped\": true_direct_mapped,\n",
    "            \"true_indirect_mapped\": true_indirect_mapped,\n",
    "            \"pred_direct_mapped\": pred_direct_mapped,\n",
    "            \"pred_indirect_mapped\": pred_indirect_mapped\n",
    "        })\n",
    "    \n",
    "    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡\n",
    "    print(\"ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...\")\n",
    "    results = {\n",
    "        \"direct\": calculate_classification_metrics(\n",
    "            evaluation_data[\"direct\"][\"true\"], \n",
    "            evaluation_data[\"direct\"][\"pred\"], \n",
    "            DIRECT_CAUSES\n",
    "        ),\n",
    "        \"indirect\": calculate_classification_metrics(\n",
    "            evaluation_data[\"indirect\"][\"true\"], \n",
    "            evaluation_data[\"indirect\"][\"pred\"], \n",
    "            INDIRECT_CAUSES\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\n===== è¯„ä¼°ç»“æœ =====\")\n",
    "    print(f\"ç›´æ¥åŸå› ç²¾ç¡®ç‡: {results['direct']['overall']['precision']:.4f}\")\n",
    "    print(f\"ç›´æ¥åŸå› å¬å›ç‡: {results['direct']['overall']['recall']:.4f}\")\n",
    "    print(f\"ç›´æ¥åŸå› F1: {results['direct']['overall']['f1']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› ç²¾ç¡®ç‡: {results['indirect']['overall']['precision']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› å¬å›ç‡: {results['indirect']['overall']['recall']:.4f}\")\n",
    "    print(f\"é—´æ¥åŸå› F1: {results['indirect']['overall']['f1']:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´ç»“æœ\n",
    "    output_path = csv_path.replace(\".csv\", \"_evaluation_results.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"metrics\": results,\n",
    "            \"details\": evaluation_data[\"details\"]\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… è¯„ä¼°å®Œæˆ! è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "if __name__ == \"__main__\":\n",
    "    # CSVæ–‡ä»¶è·¯å¾„\n",
    "    csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\å›¾ï¼Œè¡¨\\supply_chain_accidents_data_with_causes.csv\"\n",
    "    \n",
    "    # è¿è¡Œè¯„ä¼°\n",
    "    results = evaluate_csv(csv_path)\n",
    "    \n",
    "    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡\n",
    "    if results:\n",
    "        print(\"\\n===== ç›´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\")\n",
    "        for cause, metrics in results[\"direct\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}\")\n",
    "            print(f\"  å¬å›ç‡: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  æ”¯æŒåº¦: {metrics['support']}\")\n",
    "        \n",
    "        print(\"\\n===== é—´æ¥åŸå› è¯¦ç»†æŒ‡æ ‡ =====\")\n",
    "        for cause, metrics in results[\"indirect\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}\")\n",
    "            print(f\"  å¬å›ç‡: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  æ”¯æŒåº¦: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c8f33-68e7-451b-a22e-cf0087076bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finetuning)",
   "language": "python",
   "name": "finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2773c07e-4c46-4fcc-b19f-8fc716554b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🔍 适配器路径验证: D:/ft/qwen-14b-1000-new/fine_tuned_model (包含 9 个文件)\n",
      "⏳ 加载基础模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeslieHailee\\anaconda3\\envs\\finetuning\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.842 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a791c41eb37b409c82ef673205bf2243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 加载PEFT适配器...\n",
      "🔠 加载分词器...\n",
      "⚙️ 创建推理管道...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 读取CSV文件: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents.csv\n",
      "🚀 开始处理事故描述...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:   6%|▌         | 5/82 [01:29<21:53, 17.06s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (5/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  11%|█         | 9/82 [02:36<20:03, 16.49s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (10/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  12%|█▏        | 10/82 [02:51<19:03, 15.88s/行]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  18%|█▊        | 15/82 [04:25<20:39, 18.49s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (15/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  23%|██▎       | 19/82 [05:46<22:24, 21.35s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (20/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  24%|██▍       | 20/82 [06:04<21:12, 20.53s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  30%|███       | 25/82 [07:36<18:38, 19.63s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (25/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  35%|███▌      | 29/82 [08:39<14:34, 16.50s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (30/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  37%|███▋      | 30/82 [09:02<15:59, 18.45s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  43%|████▎     | 35/82 [10:35<14:46, 18.85s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (35/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  48%|████▊     | 39/82 [11:57<14:08, 19.74s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (40/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  49%|████▉     | 40/82 [12:13<13:04, 18.67s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  55%|█████▍    | 45/82 [13:38<10:41, 17.33s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (45/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  60%|█████▉    | 49/82 [14:54<10:11, 18.53s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (50/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  61%|██████    | 50/82 [15:12<09:53, 18.56s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  67%|██████▋   | 55/82 [16:42<07:42, 17.15s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (55/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  72%|███████▏  | 59/82 [18:08<07:52, 20.53s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (60/82)\n",
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  79%|███████▉  | 65/82 [19:51<04:51, 17.15s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (65/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  84%|████████▍ | 69/82 [21:10<03:56, 18.22s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (70/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  85%|████████▌ | 70/82 [21:24<03:23, 16.95s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  91%|█████████▏| 75/82 [22:53<02:00, 17.16s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (75/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据:  96%|█████████▋| 79/82 [24:10<00:53, 17.94s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 已保存中间结果 (80/82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理事故数据: 100%|██████████| 82/82 [24:30<00:00, 17.93s/行]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU缓存\n",
      "✅ 处理完成! 共处理 80 条记录，结果保存至: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# 配置设置\n",
    "base_model_name = \"unsloth/Qwen3-14B\"\n",
    "adapter_path = \"D:/ft/qwen-14b-1000-new/fine_tuned_model\"  # 适配器目录\n",
    "csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents.csv\"  # 替换为您的CSV文件路径\n",
    "output_csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents_data.csv\"  # 替换为输出文件路径\n",
    "batch_size = 1  # 批处理大小\n",
    "\n",
    "\n",
    "# 1. 加载模型和分词器\n",
    "def load_model():\n",
    "    print(\"⏳ 加载基础模型...\")\n",
    "    # 先加载基础模型\n",
    "    base_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_name,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"🔄 加载PEFT适配器...\")\n",
    "    # 使用标准的PeftModel加载适配器\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        adapter_name=\"accident_cause_adapter\"\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"🔠 加载分词器...\")\n",
    "    # 从适配器目录加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        adapter_path,\n",
    "        padding_side=\"left\",\n",
    "        truncation_side=\"left\"\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 2. 创建推理管道 - 修正设备问题\n",
    "def create_inference_pipeline(model, tokenizer):\n",
    "    print(\"⚙️ 创建推理管道...\")\n",
    "    # 移除device参数，因为模型已通过device_map自动分配到设备\n",
    "    return pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# 3. 生成响应的函数\n",
    "def generate_response(pipe, input_text):\n",
    "    try:\n",
    "        # 构建Alpaca格式提示\n",
    "        prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "你是一个事故因果推理专家，需要根据input中的事故过程从给出的直接原因和间接原因分类表中推理出事故对应的直接原因和间接原因，输出只能为：思维链。直接原因:直接原因列表，间接原因:间接原因列表。直接原因分类表有:车辆操作安全意识淡薄,超速与载重合规性,高风险路段驾驶行为不合规,重型车辆操作规范,车辆行驶稳定性管理,车间装卸货物规范安全操作,工地卸载货物操作不规范.间接原因分类表有:施工现场人员安全管理体系不健全,运输过程合规性监管失效,重型车辆驾驶员运输培训不到位,企业管理车辆驾驶规范责任失效。\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        formatted_prompt = prompt_template.format(input_text)\n",
    "        \n",
    "        # 生成参数\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\":2048,  # 减少生成长度以节省内存\n",
    "            \"temperature\": 0.6,    # 降低随机性\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 30,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": pipe.tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.2\n",
    "        }\n",
    "        \n",
    "        # 生成响应\n",
    "        outputs = pipe(\n",
    "            formatted_prompt,\n",
    "            **generation_config,\n",
    "            return_full_text=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # 提取响应内容\n",
    "        response = outputs[0]['generated_text'].strip()\n",
    "        \n",
    "        # 清理响应内容\n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        # 移除可能的重复内容\n",
    "        response = re.sub(r'(直接原因|间接原因).*?[:：]', '', response, count=1)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 生成错误: {str(e)}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# 4. 主处理流程\n",
    "def process_csv(csv_path, output_path):\n",
    "    # 加载模型\n",
    "    model, tokenizer = load_model()\n",
    "    pipe = create_inference_pipeline(model, tokenizer)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    print(f\"📖 读取CSV文件: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取CSV失败: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # 确保存在目标列\n",
    "    if \"事故中断过程描述\" not in df.columns:\n",
    "        print(\"❌ CSV文件中缺少'事故中断过程描述'列\")\n",
    "        return\n",
    "    \n",
    "    # 添加结果列\n",
    "    df[\"模型推理结果\"] = \"\"\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"🚀 开始处理事故描述...\")\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 使用tqdm创建进度条\n",
    "    progress_bar = tqdm(total=total_rows, desc=\"处理事故数据\", unit=\"行\")\n",
    "    \n",
    "    for i in range(total_rows):\n",
    "        accident_desc = df.at[i, \"事故中断过程描述\"]\n",
    "        \n",
    "        # 跳过空描述\n",
    "        if pd.isna(accident_desc) or str(accident_desc).strip() == \"\":\n",
    "            df.at[i, \"模型推理结果\"] = \"SKIPPED: 空描述\"\n",
    "            progress_bar.update(1)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # 生成推理结果\n",
    "            result = generate_response(pipe, str(accident_desc))\n",
    "            df.at[i, \"模型推理结果\"] = result\n",
    "            processed_count += 1\n",
    "            \n",
    "            # 每处理5行保存一次中间结果\n",
    "            if processed_count > 0 and processed_count % 5 == 0:\n",
    "                df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"💾 已保存中间结果 ({processed_count}/{total_rows})\")\n",
    "            \n",
    "            # 每处理10行清理一次内存\n",
    "            if processed_count > 0 and processed_count % 10 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(\"🧹 已清理GPU缓存\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 行 {i+1} 处理失败: {str(e)}\")\n",
    "            df.at[i, \"模型推理结果\"] = f\"ERROR: {str(e)}\"\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # 最终保存\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 处理完成! 共处理 {processed_count} 条记录，结果保存至: {output_path}\")\n",
    "    \n",
    "    # 释放资源\n",
    "    del model, tokenizer, pipe\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 执行处理\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查路径是否存在\n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(f\"❌ 适配器路径不存在: {adapter_path}\")\n",
    "    else:\n",
    "        print(f\"🔍 适配器路径验证: {adapter_path} (包含 {len(os.listdir(adapter_path))} 个文件)\")\n",
    "    \n",
    "    # 设置环境变量防止并行处理冲突\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # 设置CUDA设备可见性\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 指定使用第一块GPU\n",
    "    \n",
    "    process_csv(csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff7d10d-abe3-4f33-af68-cf614139ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 读取CSV文件: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents_data_with_causes.csv\n",
      "🔍 解析原因并映射到分类表...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理行: 100%|██████████| 82/82 [00:00<00:00, 40998.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 计算评估指标...\n",
      "\n",
      "===== 评估结果 =====\n",
      "直接原因精确率: 0.6992\n",
      "直接原因召回率: 0.6509\n",
      "直接原因F1: 0.6669\n",
      "间接原因精确率: 0.6389\n",
      "间接原因召回率: 0.6065\n",
      "间接原因F1: 0.6028\n",
      "\n",
      "✅ 评估完成! 详细结果已保存至: E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents_data_with_causes_evaluation_results.json\n",
      "\n",
      "===== 直接原因详细指标 =====\n",
      "车辆操作安全意识淡薄:\n",
      "  精确率: 0.5000\n",
      "  召回率: 0.1875\n",
      "  F1: 0.2727\n",
      "  支持度: 16\n",
      "超速与载重合规性:\n",
      "  精确率: 0.5000\n",
      "  召回率: 0.5833\n",
      "  F1: 0.5385\n",
      "  支持度: 12\n",
      "高风险路段驾驶行为不合规:\n",
      "  精确率: 0.8333\n",
      "  召回率: 0.2778\n",
      "  F1: 0.4167\n",
      "  支持度: 18\n",
      "重型车辆操作规范:\n",
      "  精确率: 0.5556\n",
      "  召回率: 0.2174\n",
      "  F1: 0.3125\n",
      "  支持度: 23\n",
      "车辆行驶稳定性管理:\n",
      "  精确率: 0.5000\n",
      "  召回率: 0.3000\n",
      "  F1: 0.3750\n",
      "  支持度: 20\n",
      "车间装卸货物规范安全操作:\n",
      "  精确率: 0.7000\n",
      "  召回率: 0.3889\n",
      "  F1: 0.5000\n",
      "  支持度: 18\n",
      "工地卸载货物操作不规范:\n",
      "  精确率: 0.5161\n",
      "  召回率: 0.8421\n",
      "  F1: 0.6400\n",
      "  支持度: 19\n",
      "\n",
      "===== 间接原因详细指标 =====\n",
      "施工现场人员安全管理体系不健全:\n",
      "  精确率: 0.6000\n",
      "  召回率: 0.4737\n",
      "  F1: 0.5294\n",
      "  支持度: 38\n",
      "运输过程合规性监管失效:\n",
      "  精确率: 0.2857\n",
      "  召回率: 0.0870\n",
      "  F1: 0.1333\n",
      "  支持度: 23\n",
      "重型车辆驾驶员运输培训不到位:\n",
      "  精确率: 0.8000\n",
      "  召回率: 0.1250\n",
      "  F1: 0.2162\n",
      "  支持度: 32\n",
      "企业管理车辆驾驶规范责任失效:\n",
      "  精确率: 0.6579\n",
      "  召回率: 0.6410\n",
      "  F1: 0.6494\n",
      "  支持度: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义原因分类表\n",
    "DIRECT_CAUSES = [\n",
    "    \"车辆操作安全意识淡薄\",\n",
    "    \"超速与载重合规性\",\n",
    "    \"高风险路段驾驶行为不合规\",\n",
    "    \"重型车辆操作规范\",\n",
    "    \"车辆行驶稳定性管理\",\n",
    "    \"车间装卸货物规范安全操作\",\n",
    "    \"工地卸载货物操作不规范\"\n",
    "]\n",
    "\n",
    "INDIRECT_CAUSES = [\n",
    "    \"施工现场人员安全管理体系不健全\",\n",
    "    \"运输过程合规性监管失效\",\n",
    "    \"重型车辆驾驶员运输培训不到位\",\n",
    "    \"企业管理车辆驾驶规范责任失效\"\n",
    "]\n",
    "\n",
    "class RobustCauseParser:\n",
    "    \"\"\"鲁棒的原因解析器，处理不同格式的原因描述\"\"\"\n",
    "    \n",
    "    def extract_causes(self, text):\n",
    "        \"\"\"从文本中提取原因列表\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return [], []\n",
    "            \n",
    "        # 尝试从文本中提取原因\n",
    "        direct_list = []\n",
    "        indirect_list = []\n",
    "        \n",
    "        # 尝试匹配格式化的原因列表\n",
    "        if \"直接原因:\" in text and \"间接原因:\" in text:\n",
    "            try:\n",
    "                # 尝试提取直接原因部分\n",
    "                direct_part = re.search(r\"直接原因[:：]\\s*(.+?)(间接原因|$)\", text, re.DOTALL)\n",
    "                if direct_part:\n",
    "                    direct_str = direct_part.group(1).strip()\n",
    "                    direct_list = [c.strip() for c in re.split(r\"[,，]\", direct_str) if c.strip()]\n",
    "                \n",
    "                # 尝试提取间接原因部分\n",
    "                indirect_part = re.search(r\"间接原因[:：]\\s*(.+)\", text)\n",
    "                if indirect_part:\n",
    "                    indirect_str = indirect_part.group(1).strip()\n",
    "                    indirect_list = [c.strip() for c in re.split(r\"[,，]\", indirect_str) if c.strip()]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 如果没有匹配到格式化的原因，尝试从自由文本中匹配\n",
    "        if not direct_list:\n",
    "            for cause in DIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    direct_list.append(cause)\n",
    "        \n",
    "        if not indirect_list:\n",
    "            for cause in INDIRECT_CAUSES:\n",
    "                if cause in text:\n",
    "                    indirect_list.append(cause)\n",
    "        \n",
    "        return direct_list, indirect_list\n",
    "\n",
    "class CauseMapper:\n",
    "    \"\"\"将原始原因映射到标准分类表\"\"\"\n",
    "    \n",
    "    def __init__(self, direct_causes, indirect_causes):\n",
    "        self.direct_causes = direct_causes\n",
    "        self.indirect_causes = indirect_causes\n",
    "        \n",
    "    def map_direct(self, causes):\n",
    "        \"\"\"将原始直接原因映射到标准分类\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            # 尝试完全匹配\n",
    "            if cause in self.direct_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            # 尝试部分匹配\n",
    "            for standard_cause in self.direct_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))  # 去重\n",
    "    \n",
    "    def map_indirect(self, causes):\n",
    "        \"\"\"将原始间接原因映射到标准分类\"\"\"\n",
    "        mapped = []\n",
    "        for cause in causes:\n",
    "            if cause in self.indirect_causes:\n",
    "                mapped.append(cause)\n",
    "                continue\n",
    "                \n",
    "            for standard_cause in self.indirect_causes:\n",
    "                if standard_cause in cause or cause in standard_cause:\n",
    "                    mapped.append(standard_cause)\n",
    "                    break\n",
    "        return list(set(mapped))\n",
    "\n",
    "def calculate_classification_metrics(true_labels, pred_labels, class_list):\n",
    "    \"\"\"计算分类指标（宏平均）\"\"\"\n",
    "    # 将标签列表转换为二进制向量\n",
    "    true_vectors = []\n",
    "    pred_vectors = []\n",
    "    \n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_vec = [1 if c in true else 0 for c in class_list]\n",
    "        pred_vec = [1 if c in pred else 0 for c in class_list]\n",
    "        true_vectors.append(true_vec)\n",
    "        pred_vectors.append(pred_vec)\n",
    "    \n",
    "    # 展平向量用于整体指标计算\n",
    "    flat_true = np.array(true_vectors).flatten()\n",
    "    flat_pred = np.array(pred_vectors).flatten()\n",
    "    \n",
    "    # 计算整体指标\n",
    "    precision = precision_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(flat_true, flat_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # 计算每个类别的指标\n",
    "    class_metrics = {}\n",
    "    for i, cause in enumerate(class_list):\n",
    "        class_true = [vec[i] for vec in true_vectors]\n",
    "        class_pred = [vec[i] for vec in pred_vectors]\n",
    "        \n",
    "        class_metrics[cause] = {\n",
    "            \"precision\": precision_score(class_true, class_pred, zero_division=0),\n",
    "            \"recall\": recall_score(class_true, class_pred, zero_division=0),\n",
    "            \"f1\": f1_score(class_true, class_pred, zero_division=0),\n",
    "            \"support\": sum(class_true)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"overall\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        },\n",
    "        \"per_class\": class_metrics\n",
    "    }\n",
    "\n",
    "def evaluate_csv(csv_path):\n",
    "    \"\"\"评估CSV文件中的预测结果\"\"\"\n",
    "    # 读取CSV文件\n",
    "    print(f\"📖 读取CSV文件: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取CSV失败: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # 检查必要列是否存在\n",
    "    required_columns = ['直接原因', '间接原因', '匹配的直接原因', '匹配的间接原因']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"❌ CSV文件中缺少'{col}'列\")\n",
    "            return None\n",
    "    \n",
    "    # 初始化工具\n",
    "    parser = RobustCauseParser()\n",
    "    mapper = CauseMapper(DIRECT_CAUSES, INDIRECT_CAUSES)\n",
    "    \n",
    "    # 准备评估数据容器\n",
    "    evaluation_data = {\n",
    "        \"direct\": {\"true\": [], \"pred\": []},\n",
    "        \"indirect\": {\"true\": [], \"pred\": []},\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    print(\"🔍 解析原因并映射到分类表...\")\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"处理行\"):\n",
    "        # 解析真实原因\n",
    "        true_direct_raw = str(row[\"直接原因\"]) if not pd.isna(row[\"直接原因\"]) else \"\"\n",
    "        true_indirect_raw = str(row[\"间接原因\"]) if not pd.isna(row[\"间接原因\"]) else \"\"\n",
    "        \n",
    "        # 解析预测原因\n",
    "        pred_direct_raw = str(row[\"匹配的直接原因\"]) if not pd.isna(row[\"匹配的直接原因\"]) else \"\"\n",
    "        pred_indirect_raw = str(row[\"匹配的间接原因\"]) if not pd.isna(row[\"匹配的间接原因\"]) else \"\"\n",
    "        \n",
    "        # 映射到分类表\n",
    "        true_direct_mapped = mapper.map_direct(parser.extract_causes(true_direct_raw)[0])\n",
    "        true_indirect_mapped = mapper.map_indirect(parser.extract_causes(true_indirect_raw)[1])\n",
    "        pred_direct_mapped = mapper.map_direct(parser.extract_causes(pred_direct_raw)[0])\n",
    "        pred_indirect_mapped = mapper.map_indirect(parser.extract_causes(pred_indirect_raw)[1])\n",
    "        \n",
    "        # 存储结果\n",
    "        evaluation_data[\"direct\"][\"true\"].append(true_direct_mapped)\n",
    "        evaluation_data[\"direct\"][\"pred\"].append(pred_direct_mapped)\n",
    "        evaluation_data[\"indirect\"][\"true\"].append(true_indirect_mapped)\n",
    "        evaluation_data[\"indirect\"][\"pred\"].append(pred_indirect_mapped)\n",
    "        \n",
    "        # 保存详细结果用于分析\n",
    "        evaluation_data[\"details\"].append({\n",
    "            \"index\": i,\n",
    "            \"事故中断过程描述\": row[\"事故中断过程描述\"] if \"事故中断过程描述\" in row else \"\",\n",
    "            \"true_direct_raw\": true_direct_raw,\n",
    "            \"true_indirect_raw\": true_indirect_raw,\n",
    "            \"pred_direct_raw\": pred_direct_raw,\n",
    "            \"pred_indirect_raw\": pred_indirect_raw,\n",
    "            \"true_direct_mapped\": true_direct_mapped,\n",
    "            \"true_indirect_mapped\": true_indirect_mapped,\n",
    "            \"pred_direct_mapped\": pred_direct_mapped,\n",
    "            \"pred_indirect_mapped\": pred_indirect_mapped\n",
    "        })\n",
    "    \n",
    "    # 计算分类指标\n",
    "    print(\"📊 计算评估指标...\")\n",
    "    results = {\n",
    "        \"direct\": calculate_classification_metrics(\n",
    "            evaluation_data[\"direct\"][\"true\"], \n",
    "            evaluation_data[\"direct\"][\"pred\"], \n",
    "            DIRECT_CAUSES\n",
    "        ),\n",
    "        \"indirect\": calculate_classification_metrics(\n",
    "            evaluation_data[\"indirect\"][\"true\"], \n",
    "            evaluation_data[\"indirect\"][\"pred\"], \n",
    "            INDIRECT_CAUSES\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\n===== 评估结果 =====\")\n",
    "    print(f\"直接原因精确率: {results['direct']['overall']['precision']:.4f}\")\n",
    "    print(f\"直接原因召回率: {results['direct']['overall']['recall']:.4f}\")\n",
    "    print(f\"直接原因F1: {results['direct']['overall']['f1']:.4f}\")\n",
    "    print(f\"间接原因精确率: {results['indirect']['overall']['precision']:.4f}\")\n",
    "    print(f\"间接原因召回率: {results['indirect']['overall']['recall']:.4f}\")\n",
    "    print(f\"间接原因F1: {results['indirect']['overall']['f1']:.4f}\")\n",
    "    \n",
    "    # 保存完整结果\n",
    "    output_path = csv_path.replace(\".csv\", \"_evaluation_results.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"metrics\": results,\n",
    "            \"details\": evaluation_data[\"details\"]\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ 评估完成! 详细结果已保存至: {output_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 执行评估\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV文件路径\n",
    "    csv_path = r\"E:\\SEU\\construction organizational resilience framework\\supply chain resilience indicators\\paper\\图，表\\supply_chain_accidents_data_with_causes.csv\"\n",
    "    \n",
    "    # 运行评估\n",
    "    results = evaluate_csv(csv_path)\n",
    "    \n",
    "    # 打印每个类别的详细指标\n",
    "    if results:\n",
    "        print(\"\\n===== 直接原因详细指标 =====\")\n",
    "        for cause, metrics in results[\"direct\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  精确率: {metrics['precision']:.4f}\")\n",
    "            print(f\"  召回率: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  支持度: {metrics['support']}\")\n",
    "        \n",
    "        print(\"\\n===== 间接原因详细指标 =====\")\n",
    "        for cause, metrics in results[\"indirect\"][\"per_class\"].items():\n",
    "            print(f\"{cause}:\")\n",
    "            print(f\"  精确率: {metrics['precision']:.4f}\")\n",
    "            print(f\"  召回率: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "            print(f\"  支持度: {metrics['support']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c8f33-68e7-451b-a22e-cf0087076bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finetuning)",
   "language": "python",
   "name": "finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
